{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank Customer Churn Prediction using Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Statement:** Given a bank customer, build a neural network-based classifier that can determine whether they will leave or not in the next 6 months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objectives:**\n",
    "1. Read and pre-process the dataset.\n",
    "2. Distinguish the feature and target set and divide the data into training and test sets.\n",
    "3. Normalize the feature data.\n",
    "4. Initialize, build, and train the neural network model.\n",
    "5. Evaluate the model using accuracy score and confusion matrix.\n",
    "6. Identify points for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Import necessary libraries for data manipulation, visualization, and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Churn_Modelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           10000 non-null  int64  \n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(2), int64(9), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Check data types and non-null counts\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Clean Data\n",
    "Drop unnecessary columns like 'Email No.' which is just an identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"RowNumber\", \"CustomerId\", \"Surname\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Feature and Target Set\n",
    "Define Features (X) and Target (y):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Exited' is the target column\n",
    "X = df.drop(\"Exited\", axis=1)\n",
    "y = df[\"Exited\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Feature and Target Set\n",
    "Check target variable distribution for imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exited\n",
      "0    0.7963\n",
      "1    0.2037\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Train-Test Split\n",
    "Split the data into training and testing sets (80% train, 20% test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set shape: (8000, 10)\n",
      "Test set shape: (2000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Identify numerical and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
    "categorical_features = ['Geography', 'Gender']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Encoding Categorical Data\n",
    "Converts categorical features (Geography, Gender) into numerical dummy variables. We use drop='first' to avoid multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(handle_unknown='ignore', drop='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded = encoder.fit_transform(X_train[categorical_features]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_encoded = encoder.transform(X_test[categorical_features]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Normalize the Data\n",
    "Normalizes the numerical features by removing the mean and scaling to unit variance. This is essential for neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.fit_transform(X_train[numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test[numerical_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Combine processed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed = np.hstack((X_train_scaled, X_train_encoded))\n",
    "X_test_processed = np.hstack((X_test_scaled, X_test_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train_processed.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 11)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_processed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialize and Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\ml-env\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Adding the input and first hidden layer\n",
    "model.add(Dense(units=6, activation='relu', input_dim=input_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the second hidden layer\n",
    "model.add(Dense(units=6, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the output layer\n",
    "model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the ANN\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7386 - loss: 0.5476\n",
      "Epoch 2/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7960 - loss: 0.4515  \n",
      "Epoch 3/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8004 - loss: 0.4350  \n",
      "Epoch 4/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8074 - loss: 0.4272\n",
      "Epoch 5/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8139 - loss: 0.4213\n",
      "Epoch 6/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8173 - loss: 0.4154  \n",
      "Epoch 7/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8230 - loss: 0.4087  \n",
      "Epoch 8/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8280 - loss: 0.4019\n",
      "Epoch 9/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8301 - loss: 0.3935  \n",
      "Epoch 10/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8316 - loss: 0.3870\n",
      "Epoch 11/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8369 - loss: 0.3818\n",
      "Epoch 12/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8422 - loss: 0.3765\n",
      "Epoch 13/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8449 - loss: 0.3723  \n",
      "Epoch 14/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8469 - loss: 0.3685\n",
      "Epoch 15/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8503 - loss: 0.3655\n",
      "Epoch 16/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8505 - loss: 0.3629\n",
      "Epoch 17/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8515 - loss: 0.3605  \n",
      "Epoch 18/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8534 - loss: 0.3583 \n",
      "Epoch 19/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8535 - loss: 0.3573  \n",
      "Epoch 20/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8534 - loss: 0.3549\n",
      "Epoch 21/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8546 - loss: 0.3530  \n",
      "Epoch 22/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8559 - loss: 0.3518\n",
      "Epoch 23/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8568 - loss: 0.3504  \n",
      "Epoch 24/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8575 - loss: 0.3496  \n",
      "Epoch 25/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8572 - loss: 0.3485\n",
      "Epoch 26/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8559 - loss: 0.3483\n",
      "Epoch 27/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8570 - loss: 0.3476\n",
      "Epoch 28/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8565 - loss: 0.3468  \n",
      "Epoch 29/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8576 - loss: 0.3460  \n",
      "Epoch 30/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8571 - loss: 0.3458\n",
      "Epoch 31/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8565 - loss: 0.3449\n",
      "Epoch 32/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8559 - loss: 0.3451  \n",
      "Epoch 33/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8571 - loss: 0.3449  \n",
      "Epoch 34/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8561 - loss: 0.3443\n",
      "Epoch 35/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8581 - loss: 0.3435\n",
      "Epoch 36/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8579 - loss: 0.3432   \n",
      "Epoch 37/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8581 - loss: 0.3428\n",
      "Epoch 38/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8580 - loss: 0.3426  \n",
      "Epoch 39/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8570 - loss: 0.3418\n",
      "Epoch 40/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8586 - loss: 0.3424\n",
      "Epoch 41/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8584 - loss: 0.3422  \n",
      "Epoch 42/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8590 - loss: 0.3414  \n",
      "Epoch 43/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8558 - loss: 0.3419\n",
      "Epoch 44/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8587 - loss: 0.3410  \n",
      "Epoch 45/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8580 - loss: 0.3412\n",
      "Epoch 46/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8593 - loss: 0.3407\n",
      "Epoch 47/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8576 - loss: 0.3411  \n",
      "Epoch 48/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8593 - loss: 0.3403  \n",
      "Epoch 49/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8586 - loss: 0.3399  \n",
      "Epoch 50/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8597 - loss: 0.3405  \n",
      "Epoch 51/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8586 - loss: 0.3401  \n",
      "Epoch 52/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8597 - loss: 0.3395  \n",
      "Epoch 53/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8601 - loss: 0.3392  \n",
      "Epoch 54/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8576 - loss: 0.3393\n",
      "Epoch 55/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8596 - loss: 0.3395  \n",
      "Epoch 56/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8594 - loss: 0.3389  \n",
      "Epoch 57/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8609 - loss: 0.3393\n",
      "Epoch 58/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8586 - loss: 0.3395\n",
      "Epoch 59/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8600 - loss: 0.3392\n",
      "Epoch 60/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8595 - loss: 0.3389\n",
      "Epoch 61/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8583 - loss: 0.3388\n",
      "Epoch 62/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8587 - loss: 0.3385\n",
      "Epoch 63/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8596 - loss: 0.3383\n",
      "Epoch 64/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8585 - loss: 0.3385\n",
      "Epoch 65/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8595 - loss: 0.3388\n",
      "Epoch 66/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8595 - loss: 0.3383\n",
      "Epoch 67/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8579 - loss: 0.3383\n",
      "Epoch 68/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8596 - loss: 0.3384\n",
      "Epoch 69/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8596 - loss: 0.3377\n",
      "Epoch 70/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8602 - loss: 0.3379\n",
      "Epoch 71/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8587 - loss: 0.3382\n",
      "Epoch 72/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8595 - loss: 0.3380\n",
      "Epoch 73/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8605 - loss: 0.3380\n",
      "Epoch 74/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8571 - loss: 0.3387\n",
      "Epoch 75/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8612 - loss: 0.3376\n",
      "Epoch 76/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8584 - loss: 0.3374\n",
      "Epoch 77/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8595 - loss: 0.3375\n",
      "Epoch 78/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8597 - loss: 0.3372\n",
      "Epoch 79/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8601 - loss: 0.3375\n",
      "Epoch 80/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8602 - loss: 0.3381\n",
      "Epoch 81/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8580 - loss: 0.3376\n",
      "Epoch 82/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8621 - loss: 0.3373\n",
      "Epoch 83/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8604 - loss: 0.3377\n",
      "Epoch 84/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8612 - loss: 0.3374\n",
      "Epoch 85/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8597 - loss: 0.3370\n",
      "Epoch 86/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8590 - loss: 0.3377\n",
      "Epoch 87/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8610 - loss: 0.3368\n",
      "Epoch 88/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8594 - loss: 0.3375\n",
      "Epoch 89/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8596 - loss: 0.3372\n",
      "Epoch 90/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8591 - loss: 0.3371\n",
      "Epoch 91/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8602 - loss: 0.3374  \n",
      "Epoch 92/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8594 - loss: 0.3374\n",
      "Epoch 93/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8602 - loss: 0.3370\n",
      "Epoch 94/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8599 - loss: 0.3378\n",
      "Epoch 95/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8606 - loss: 0.3378\n",
      "Epoch 96/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8594 - loss: 0.3366\n",
      "Epoch 97/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8631 - loss: 0.3369\n",
      "Epoch 98/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8596 - loss: 0.3365  \n",
      "Epoch 99/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8595 - loss: 0.3373  \n",
      "Epoch 100/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8611 - loss: 0.3367  \n"
     ]
    }
   ],
   "source": [
    "# Training the ANN\n",
    "history = model.fit( X_train_processed, y_train, batch_size=32, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Print the Accuracy Score and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_probs = model.predict(X_test_processed)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Accuracy: 0.8605\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTest Set Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1537   70]\n",
      " [ 209  184]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(58.222222222222214, 0.5, 'Actual')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot confusion matrix (optional)\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Stays', 'Exits'], yticklabels=['Stays', 'Exits'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Points of Improvement\n",
    "\n",
    "The current model provides a good baseline, but several improvements can be implemented:\n",
    "\n",
    "1.  **Hyperparameter Tuning:** Experiment with different numbers of hidden layers, neurons per layer, and activation functions. The number of neurons (6) in the hidden layers is a common starting point (average of input and output layers), but may not be optimal.\n",
    "\n",
    "2.  **Regularization:** To prevent overfitting, especially with more complex models, dropout layers (`from tensorflow.keras.layers import Dropout`) can be added between the hidden layers. This randomly deactivates a fraction of neurons during training, forcing the network to learn more robust features.\n",
    "\n",
    "3.  **Optimizer and Learning Rate:** While 'adam' is a robust default optimizer, experimenting with others like 'RMSprop' or 'SGD' with momentum could yield better results. The learning rate of the optimizer can also be tuned.\n",
    "\n",
    "4.  **Batch Size and Epochs:** Adjusting the batch size and the number of training epochs can impact performance and training time. The model might benefit from more epochs if it is underfitting, or fewer if it is overfitting. Early stopping can be used to monitor validation loss and stop training when it no longer improves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Build the Improved Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Calculate Class Weights for imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculated Class Weights: {0: np.float64(0.6293266205160478), 1: np.float64(2.4330900243309004)}\n",
      "Class 0 (Stays) will be weighted less, Class 1 (Exits) will be weighted more.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nCalculated Class Weights: {class_weights_dict}\")\n",
    "print(\"Class 0 (Stays) will be weighted less, Class 1 (Exits) will be weighted more.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Build Improved Model with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\ml-env\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model_improved = Sequential()\n",
    "model_improved.add(Dense(units=12, activation='relu', input_dim=input_dim))\n",
    "model_improved.add(Dropout(0.3))  # Add dropout (30%)\n",
    "model_improved.add(Dense(units=8, activation='relu'))\n",
    "model_improved.add(Dropout(0.3))  # Add dropout (30%)\n",
    "model_improved.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the improved model\n",
    "model_improved.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7956 - loss: 0.4710\n",
      "Epoch 2/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7928 - loss: 0.4571\n",
      "Epoch 3/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7889 - loss: 0.4579  \n",
      "Epoch 4/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7900 - loss: 0.4553\n",
      "Epoch 5/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7854 - loss: 0.4562\n",
      "Epoch 6/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7865 - loss: 0.4551\n",
      "Epoch 7/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7870 - loss: 0.4546\n",
      "Epoch 8/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7878 - loss: 0.4549\n",
      "Epoch 9/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7885 - loss: 0.4547  \n",
      "Epoch 10/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7879 - loss: 0.4547  \n",
      "Epoch 11/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7914 - loss: 0.4540\n",
      "Epoch 12/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7922 - loss: 0.4543  \n",
      "Epoch 13/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7886 - loss: 0.4531\n",
      "Epoch 14/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7915 - loss: 0.4536\n",
      "Epoch 15/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7897 - loss: 0.4534\n",
      "Epoch 16/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7899 - loss: 0.4540\n",
      "Epoch 17/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7910 - loss: 0.4534\n",
      "Epoch 18/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7878 - loss: 0.4539\n",
      "Epoch 19/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7909 - loss: 0.4527\n",
      "Epoch 20/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7890 - loss: 0.4532\n",
      "Epoch 21/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7924 - loss: 0.4539\n",
      "Epoch 22/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7921 - loss: 0.4537\n",
      "Epoch 23/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7874 - loss: 0.4524\n",
      "Epoch 24/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7947 - loss: 0.4533\n",
      "Epoch 25/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7878 - loss: 0.4534\n",
      "Epoch 26/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7922 - loss: 0.4533\n",
      "Epoch 27/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7880 - loss: 0.4523\n",
      "Epoch 28/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7890 - loss: 0.4536\n",
      "Epoch 29/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7885 - loss: 0.4533\n",
      "Epoch 30/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7897 - loss: 0.4536\n",
      "Epoch 31/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7929 - loss: 0.4526  \n",
      "Epoch 32/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7889 - loss: 0.4539  \n",
      "Epoch 33/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7911 - loss: 0.4528  \n",
      "Epoch 34/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7910 - loss: 0.4524\n",
      "Epoch 35/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7896 - loss: 0.4527\n",
      "Epoch 36/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7910 - loss: 0.4526\n",
      "Epoch 37/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7883 - loss: 0.4523\n",
      "Epoch 38/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7921 - loss: 0.4519\n",
      "Epoch 39/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7936 - loss: 0.4521\n",
      "Epoch 40/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7878 - loss: 0.4529\n",
      "Epoch 41/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7930 - loss: 0.4515  \n",
      "Epoch 42/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7871 - loss: 0.4523  \n",
      "Epoch 43/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7920 - loss: 0.4523\n",
      "Epoch 44/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7925 - loss: 0.4522\n",
      "Epoch 45/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7889 - loss: 0.4527\n",
      "Epoch 46/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7929 - loss: 0.4520\n",
      "Epoch 47/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7914 - loss: 0.4522\n",
      "Epoch 48/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7900 - loss: 0.4526  \n",
      "Epoch 49/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7906 - loss: 0.4521\n",
      "Epoch 50/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7966 - loss: 0.4518\n",
      "Epoch 51/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7896 - loss: 0.4520\n",
      "Epoch 52/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7918 - loss: 0.4523\n",
      "Epoch 53/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7935 - loss: 0.4521\n",
      "Epoch 54/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7914 - loss: 0.4516\n",
      "Epoch 55/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7890 - loss: 0.4517\n",
      "Epoch 56/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7914 - loss: 0.4516\n",
      "Epoch 57/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7930 - loss: 0.4513\n",
      "Epoch 58/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7893 - loss: 0.4526\n",
      "Epoch 59/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7922 - loss: 0.4517\n",
      "Epoch 60/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7926 - loss: 0.4514\n",
      "Epoch 61/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7883 - loss: 0.4519\n",
      "Epoch 62/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7922 - loss: 0.4516\n",
      "Epoch 63/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7899 - loss: 0.4521\n",
      "Epoch 64/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7935 - loss: 0.4516  \n",
      "Epoch 65/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7951 - loss: 0.4509\n",
      "Epoch 66/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7900 - loss: 0.4513\n",
      "Epoch 67/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7928 - loss: 0.4521  \n",
      "Epoch 68/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7893 - loss: 0.4514  \n",
      "Epoch 69/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7904 - loss: 0.4508  \n",
      "Epoch 70/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7934 - loss: 0.4514  \n",
      "Epoch 71/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7912 - loss: 0.4520\n",
      "Epoch 72/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7900 - loss: 0.4516\n",
      "Epoch 73/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7921 - loss: 0.4513\n",
      "Epoch 74/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7945 - loss: 0.4515  \n",
      "Epoch 75/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7925 - loss: 0.4508\n",
      "Epoch 76/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7939 - loss: 0.4500\n",
      "Epoch 77/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7930 - loss: 0.4510\n",
      "Epoch 78/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7911 - loss: 0.4517\n",
      "Epoch 79/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7926 - loss: 0.4517\n",
      "Epoch 80/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7943 - loss: 0.4517\n",
      "Epoch 81/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7911 - loss: 0.4511\n",
      "Epoch 82/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7947 - loss: 0.4500\n",
      "Epoch 83/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7876 - loss: 0.4513\n",
      "Epoch 84/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7891 - loss: 0.4504\n",
      "Epoch 85/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7939 - loss: 0.4513\n",
      "Epoch 86/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7909 - loss: 0.4512\n",
      "Epoch 87/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7926 - loss: 0.4518\n",
      "Epoch 88/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7901 - loss: 0.4507\n",
      "Epoch 89/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7937 - loss: 0.4506\n",
      "Epoch 90/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7919 - loss: 0.4500\n",
      "Epoch 91/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7912 - loss: 0.4506\n",
      "Epoch 92/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7916 - loss: 0.4511\n",
      "Epoch 93/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7929 - loss: 0.4508\n",
      "Epoch 94/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7958 - loss: 0.4504\n",
      "Epoch 95/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7944 - loss: 0.4511\n",
      "Epoch 96/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7909 - loss: 0.4507\n",
      "Epoch 97/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7885 - loss: 0.4510  \n",
      "Epoch 98/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7939 - loss: 0.4509  \n",
      "Epoch 99/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7900 - loss: 0.4508\n",
      "Epoch 100/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7928 - loss: 0.4504\n"
     ]
    }
   ],
   "source": [
    "# Training the improved ANN\n",
    "history_improved = model.fit( X_train_processed, y_train, batch_size=32, epochs=100, class_weight=class_weights_dict, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Evaluate Improved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions with the improved model\n",
    "y_pred_probs_imp = model_improved.predict(X_test_processed)\n",
    "y_pred_imp = (y_pred_probs_imp > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Improved Model - Test Set Accuracy: 0.2390\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print accuracy\n",
    "accuracy_imp = accuracy_score(y_test, y_pred_imp)\n",
    "print(f\"\\nImproved Model - Test Set Accuracy: {accuracy_imp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved Model - Confusion Matrix:\n",
      "[[  95 1512]\n",
      " [  10  383]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print confusion matrix\n",
    "cm_imp = confusion_matrix(y_test, y_pred_imp)\n",
    "print(\"Improved Model - Confusion Matrix:\")\n",
    "print(cm_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Complete ---\n",
      "Initial Accuracy: 0.8605\n",
      "Improved Accuracy: 0.2390\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Evaluation Complete ---\")\n",
    "print(f\"Initial Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Improved Accuracy: {accuracy_imp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Artificial Neural Network (ANN) Concepts**\n",
    "\n",
    "An ANN is a computational model inspired by the structure and function of the human brain, composed of interconnected nodes called neurons organized in layers.\n",
    "\n",
    "#### **2.1. Keras `Sequential` Model**\n",
    "\n",
    "* **Introduction:** The `Sequential` model is the simplest way to build a model in Keras. It allows you to create a model layer-by-layer, in a linear stack.\n",
    "* **Why We Used It:** Our network was a simple feed-forward classifier, where data flows directly from the input, through the hidden layers, to the output. The `Sequential` API is the most straightforward tool for this architecture.\n",
    "\n",
    "#### **2.2. `Dense` Layer**\n",
    "\n",
    "* **Introduction:** A `Dense` layer, or a \"fully connected\" layer, is the most common type of layer in an ANN. In a `Dense` layer, every neuron is connected to every neuron in the previous layer.\n",
    "* **Why We Used It:** These layers are responsible for learning patterns. Each connection has a \"weight,\" and the layer learns by adjusting these weights during training.\n",
    "* **Properties:** The primary operation within a neuron of a `Dense` layer is:\n",
    "    $$output = activation(\\sum(weights \\times inputs) + bias)$$\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Activation Functions**\n",
    "\n",
    "Activation functions introduce non-linearity into the network, allowing it to learn complex patterns that a simple linear model could not.\n",
    "\n",
    "#### **3.1. ReLU (Rectified Linear Unit)**\n",
    "\n",
    "* **Introduction:** ReLU is the most popular activation function used in the *hidden layers* of a neural network.\n",
    "* **Why We Used It:**\n",
    "    1.  **Non-linearity:** It introduces the non-linearity needed to learn complex data.\n",
    "    2.  **Computational Efficiency:** It is very fast to compute (just a `max(0, x)` operation).\n",
    "    3.  **Mitigates Vanishing Gradients:** It does not saturate for positive inputs, which helps the model train faster and more effectively than older functions like `tanh` or `sigmoid`.\n",
    "* **Formula:**\n",
    "    $$f(x) = \\max(0, x)$$\n",
    "\n",
    "#### **3.2. Sigmoid Function**\n",
    "\n",
    "* **Introduction:** The Sigmoid function is an activation function that squashes any real-valued number into a range between $0$ and $1$.\n",
    "* **Why We Used It:** It was used *only in the output layer*. For a binary classification problem, we need the model to output a single number that represents a probability (e.g., the probability of the customer \"Exiting\"). The Sigmoid function is perfect for this, as its $0$ to $1$ output is easily interpreted as a probability.\n",
    "* **Formula:**\n",
    "    $$f(x) = \\frac{1}{1 + e^{-x}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Artificial Neural Network (ANN) Architecture**\n",
    "\n",
    "An ANN is a computing system inspired by the biological brain. It learns by finding complex patterns in data through a layered structure of interconnected \"neurons.\"\n",
    "\n",
    "#### **1.1. The Neuron (Perceptron)**\n",
    "\n",
    "* **Introduction:** The neuron is the basic computational unit of the network. It receives one or more inputs, performs a mathematical operation, and passes the result to an activation function.\n",
    "* **Operation:** A neuron calculates a \"weighted sum\" of its inputs, adds a \"bias,\" and then applies an activation function.\n",
    "* **Formula:**\n",
    "    $$y = \\text{activation} \\left( \\sum_{i=1}^{n} (w_i \\cdot x_i) + b \\right)$$\n",
    "    * $x_i$: Individual input values.\n",
    "    * $w_i$: **Weights**, which represent the *importance* or *strength* of each input. These are the main parameters the network \"learns.\"\n",
    "    * $b$: The **Bias**, a learnable parameter that allows the neuron to shift its activation function, helping it fit the data better.\n",
    "    * $y$: The neuron's output.\n",
    "\n",
    "#### **1.2. Network Layers**\n",
    "\n",
    "Neurons are organized into layers, and data flows from one layer to the next:\n",
    "\n",
    "1.  **Input Layer:** This is not a computational layer. It simply holds the $11$ processed features (e.g., scaled 'Balance', encoded 'Geography') for each customer. Its job is to pass this data to the first hidden layer.\n",
    "2.  **Hidden Layers (using `Dense`):**\n",
    "    * **Introduction:** These are the layers between the input and output. Our model used two hidden layers. A \"Dense\" layer (or \"fully connected\" layer) means that every neuron in that layer is connected to *every* neuron in the previous layer.\n",
    "    * **Purpose:** This is where the learning occurs. The first hidden layer might learn simple patterns (e.g., \"high balance and high age\"), and the next hidden layer can combine these simple patterns to learn more complex concepts (e.g., \"older customers with high balances in Germany are at risk\").\n",
    "3.  **Output Layer (using `Dense`):**\n",
    "    * **Introduction:** The final layer that produces the prediction.\n",
    "    * **Why `units=1`?** For binary classification (Exit/Stay), we only need one output neuron. This neuron's job is to output a single number.\n",
    "    * **Why `sigmoid`?** The `sigmoid` activation function squashes this number into a range between $0$ and $1$, which we can interpret as the *probability* that the customer will exit. A result of $0.85$ means the model is \"85% confident\" the customer will churn.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Activation Functions: Adding Non-Linearity**\n",
    "\n",
    "An activation function decides whether a neuron should be \"fired\" or activated. Without them, the ANN would just be a complex linear regression, unable to learn complex patterns.\n",
    "\n",
    "#### **2.1. ReLU (Rectified Linear Unit)**\n",
    "\n",
    "* **Used In:** Hidden Layers (e.g., `Dense(units=12, activation='relu')`).\n",
    "* **Formula:** $f(x) = \\max(0, x)$\n",
    "* **Explanation:** It is a very simple function. If the neuron's output ($x$) is positive, it passes it on. If it is zero or negative, it outputs $0$.\n",
    "* **Why We Used It:**\n",
    "    1.  **Solves the Vanishing Gradient Problem:** Older functions like `tanh` or `sigmoid` \"saturate\" (flatten out) for large inputs. This causes the gradient (the signal used for learning) to become extremely small, effectively \"vanishing\" and stopping the learning process in deep layers. ReLU does not saturate for positive inputs, so the learning signal remains strong.\n",
    "    2.  **Computational Efficiency:** It is extremely fast to compute (a simple `if/else` check), making training much quicker.\n",
    "\n",
    "#### **2.2. Sigmoid**\n",
    "\n",
    "* **Used In:** The *final output layer* for binary classification.\n",
    "* **Formula:** $f(x) = \\frac{1}{1 + e^{-x}}$\n",
    "* **Explanation:** It squashes any real number into a $0$-to-$1$ range.\n",
    "* **Why We Used It:** It is the standard function for binary classification because its output is a direct representation of probability, which works perfectly with the `binary_crossentropy` loss function.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. The Training Process: How the Model \"Learns\"**\n",
    "\n",
    "This process involves a \"forward pass\" and a \"backward pass\" (Backpropagation) repeated over many epochs.\n",
    "\n",
    "#### **3.1. Compilation: Defining the \"Engine\"**\n",
    "\n",
    "Before training, we must \"compile\" the model. This involves choosing three key components:\n",
    "\n",
    "1.  **Optimizer (`adam`):**\n",
    "    * **Job:** The optimizer is the algorithm that *updates the weights* ($w$) and *biases* ($b$) of the neurons to minimize the loss.\n",
    "    * **Learning Rate:** The most important hyperparameter for an optimizer is the learning rate, which controls *how large* of a step to take when updating the weights.\n",
    "    * **Why `adam`?** `adam` (Adaptive Moment Estimation) is an advanced optimizer. Its key feature is that it maintains an *adaptive learning rate* for each parameter. It learns faster for parameters that need big changes and slower for parameters that are close to optimal. This makes it highly effective and robust, often converging faster than other optimizers.\n",
    "\n",
    "2.  **Loss Function (`binary_crossentropy`):**\n",
    "    * **Job:** The loss function *measures how wrong* the model's prediction is compared to the actual answer (`y_true`).\n",
    "    * **How it Works:** It is designed to work with probabilities. It gives a high \"loss\" (penalty) for predictions that are both confident and wrong.\n",
    "    * *Example:*\n",
    "        * Actual: 1 (Exited) | Prediction: 0.9 (90% prob) $\\rightarrow$ **Low Loss**\n",
    "        * Actual: 1 (Exited) | Prediction: 0.1 (10% prob) $\\rightarrow$ **Very High Loss**\n",
    "    * The optimizer's entire goal is to adjust the weights to make this loss value as low as possible.\n",
    "\n",
    "#### **3.2. Backpropagation (The \"Learning Algorithm\")**\n",
    "\n",
    "This is the core concept of deep learning.\n",
    "\n",
    "* **Step 1: Forward Pass:** A batch of data (e.g., 32 customers) is passed *forward* through the network. The network makes 32 predictions.\n",
    "* **Step 2: Calculate Loss:** The `binary_crossentropy` function compares these 32 predictions to the 32 *actual* answers (the `y_train` values) and calculates a single loss number.\n",
    "* **Step 3: Backward Pass (Backpropagation):**\n",
    "    * The optimizer uses calculus (specifically, the chain rule) to trace the error *backward* through the network, from the output layer to the input layer.\n",
    "    * It calculates the \"gradient\" for each weight—a value that represents *how much* that single weight *contributed* to the total loss.\n",
    "    * Weights that contributed heavily to the error will get a large gradient.\n",
    "* **Step 4: Update Weights:** The `adam` optimizer uses these gradients to update every single weight and bias in the network, \"nudging\" them in the direction that will *reduce* the loss.\n",
    "* **Repeat:** This 4-step process is repeated for every batch until the entire training dataset has been seen (completing one \"epoch\"). The model is trained for many epochs.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Improving Model Generalization**\n",
    "\n",
    "#### **4.1. `Dropout` (Regularization)**\n",
    "\n",
    "* **Problem:** **Overfitting**. This is when the model \"memorizes\" the training data instead of learning the general patterns. It performs well on data it has seen but fails on new data (the test set).\n",
    "* **Solution:** We used `Dropout` layers.\n",
    "* **How it Works (During Training):** For each training step, the `Dropout(0.3)` layer *randomly deactivates* (sets to zero) 30% of the neurons from the previous layer.\n",
    "* **Why it Works:** This forces the network to learn redundant representations. It cannot rely on any single \"superstar\" neuron, as that neuron might be dropped at any moment. This makes the network's learned patterns more robust and less specific to the training data, leading to better performance on the test set. (Note: During prediction/testing, all neurons are active).\n",
    "\n",
    "#### **4.2. `class_weight='balanced'`**\n",
    "\n",
    "* **Problem:** **Class Imbalance**. Our data had ~80% \"Stays\" and ~20% \"Exits.\" A model could get ~80% accuracy by just predicting \"Stays\" every time, but it would be useless.\n",
    "* **Solution:** We used `class_weight='balanced'`.\n",
    "* **How it Works:** This technique modifies the **loss function**. It tells the model to treat misclassifications of the minority class (\"Exits\") as *more severe*. The loss for incorrectly predicting an \"Exit\" as a \"Stay\" is multiplied by a higher weight. This forces the optimizer to \"pay more attention\" to correctly identifying the rare \"Exits\" class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml-env)",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
